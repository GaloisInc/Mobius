\section{Related Work} \label{relWork}
    

Several works have been dedicated to the definition of bytecode logic.
 \cite{Quigley} defines a Hoare logics for bytecode programs. This work is limited to a subset of the Java virtual machine instructions and does not treat for example method calls, neither exceptional termination. The logic is defined by searching a structure in the bytecode control flow graph, which gives an issue to complex rules.

A work close to ours is presented in \cite{BM05plb}. The authors define a Hoare logic over a bytecode language with objects and  exceptions. A compiler from source proofs into bytecode proofs is defined. As in the present work, they assume that the bytecode has passed the bytecode verification certification. The bytecode logic aims to express functional properties. To our knowledge subroutines are not treated. Invariants are inferred by fixpoint calculation differently from the approach presented here where invariants are compiled from the high level JML specification see section \ref{comJML}. Anyways invariance ineference is not a decidable problem. Their work is inspired by~\cite{B04tlsj}. In the latter a bytecode logic is defined which checks both for well - typedness and functional correctness. ~\cite{B04tlsj} works on a stack language which does not support objects, references, exceptions and subroutines. 

In ~\cite{WildmoserN-ESOP05} a  framework is described for verifying Jinja (a Java subset) bytecode against arithmetic overflow. Anyways the annotation is written manually which is not comfortable especially on bytecode. Here we propose a way to compile a specification written in a high level language thus leaving the specification to be written on high level which is convenient.% which has the necessary expressive power to verify the same property.

The Spec\# language  developed at Microsoft, Redmond~\cite{BLS04sp} which is a superset of the C\# programming language, enriched with specification clauses,
 proposes a builtin verification framework (there is a choice to perform the checks either at runtime or statically). The static verification procedure  involves translation of the specification into metadata which is attached to the compiled bytecode and the verification is performed over the bytecode by the Boogie theorem prover.  
 
The above cited works~\cite{WildmoserN-ESOP05},~\cite{BM05plb} are targeting at extending proof carrying code(pcc) architecrture for complex safety and functional properties. Pcc and the certifying compiler  were first proposed by Necula (see \cite{Necula97}, \cite{ComNec}, \cite{DesNecLee98}). The pcc is an architecture for establishing trust in an untrusted code in which the code producer supplies a proof for correctness with the code. In this architecture it is the certifying compiler that infers automatically a type specification (e.g. loop invariants) and then generates a proof for the code correctness. Then the code issuer generates a proof over the code and the annotation and sends both to the code receiver. Code receiver then validates the proof upon its security policy. Thus the client captures (if code and proof are not both changed in a suitable way) tampering in both proof or code. The pcc architecture was designed for assembly language programs and its basic concern is to guarantee properties that are expressible via types - e.g. well typedness, read/writes are done correctly, basically these are properties that guarantee that a program do the things correctly and not that it does the right things. This problem however is resolved for bytecode languages by the java bytecode verifier which prevents the virtual machine from executing programs ill typed programs or ill formed programs. 
