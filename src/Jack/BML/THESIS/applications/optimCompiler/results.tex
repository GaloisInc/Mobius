\section{Experimental results}
\label{sec:experiments}

This section presents an application and evaluation of our method on various Java programs.

\subsection{Methodology}

We have measured the efficiency of our method on two kinds of programs, that implement features commonly met in restrained and embedded devices. \benchname{crypt} and \benchname{banking} are two smartcard-range applications. \benchname{crypt} is a cryptography benchmark from the Java Grande benchmarks suite, and \benchname{banking} is a little banking application with full JML annotations used in~\cite{BRL-JACK}. \benchname{scheduler} and \benchname{tcpip} are two embeddable system components written in Java, which are actually used in the JITS~\cite{JITSWebsite} platform. \benchname{scheduler} implements a threads scheduling mechanism, where scheduling policies are Java classes. \benchname{tcpip} is a TCP/IP stack entirely written in Java, that implements the TCP, UDP, IP, SLIP and ICMP protocols. These two components are written with low-footprint in mind ; however, the overall system performance would greatly benefit from having them available in native form, provided the memory footprint cost is not too important.

For every program, we have followed the methodology described in section \ref{sec:method} in order to prove that runtime exceptions are not thrown in these programs. We look at both the number of runtime exception check sites that we are able to remove from the native code, and the impact on the memory footprint of the natively-compiled methods with respect to the unoptimized native version and the original bytecode. The memory footprint measurements were obtained by compiling the C source file generated by the JITS AOT compiler using GCC 4.0.0 with optimization option \texttt{-Os}, for the ARM platform in thumb mode. The native methods sizes are obtained by inspecting the .o file with \texttt{nm}, and getting the size for the symbol corresponding to the native method.

Regarding the number of eliminated exception check sites, we also compare our results with the ones obtained using the JC virtual machine mentioned in~\ref{sec:relatedwork}, version 1.4.6. The results were obtained by running the \texttt{jcgen} program on the benchmark classes, and counting the number of explicit exception check sites in the generated C code. We are not comparing the memory footprints obtained with the JITS and JC AOT compilers, for this result would not be pertinent. Indeed, JC and JITS have very different ways to generate native code. JITS targets low memory footprint, and JC runtime performance. As a consequence, a runtime exception check site in JC is heavier than one in JITS, which would falsify the experiments. Suffices to say that our approach could be applied on any AOT compiler, and that the most relevant measurement is the number of runtime exception check sites that remains in the final binary - our measurements on the native code memory footprint are just here to evaluate the size impact of exception check sites.

\subsection{Results}
\label{results}
Table \ref{tab:nbexcsites} shows the results obtained on the four tested programs. The three first columns indicate the number of check sites present in the bytecode, the number of explicit check sites emitted by JC, and the number of check sites that we were unable to prove useless and that must be present in our optimized AOT code. The last columns give the memory footprints of the bytecode, unoptimized native code, and native code from which all proved exception check sites are removed.

\begin{table}
\caption{Number of exception check sites and memory footprints when compiled for ARM thumb}
\begin{center}
  \begin{tabular}{|l|r@{\extracolsep{0.2cm}}rrrrr|}
    \hline
    \multirow{2}*{Program} & \multicolumn{3}{c}{\# of exception check sites} & \multicolumn{3}{c|}{Memory footprint (bytes)}\\
    \cline{2-4} \cline{5-7} & Bytecode & ~~~~~~JC & Proven AOT & Bytecode & Naive AOT & Proven AOT\\
    \hline
    \benchname{crypt} & 190 & 79 & 1 & 1256 & 5330 & 1592\\
    \benchname{banking} & 170 & 12 & 0 & 2320 & 5634 & 3582\\
    \benchname{scheduler} & 215 & 25 & 0 & 2208 & 5416 & 2504\\
    \benchname{tcpip} & 1893 & 288 & 0 & 15497 & 41540 & 18064\\
    \hline
  \end{tabular}
\end{center}
\label{tab:nbexcsites}
\end{table}

On all the tested programs, we were able to prove that all but one exception check site could be removed. The only site that we were unable to prove from \benchname{crypt} is linked to a division, which divisor is a computed value that we were unable to prove not equal to zero. JC has to retain 16\% of all the exception check sites, with a particular mention for \benchname{crypt}, which is mainly made of array accessed and had more remaining check sites.

The memory footprints obtained clearly show the heavy overhead induced by exception check sites. Despite of the fact that the exception throwing convention has deliberately been simplified for our experiments, optimized native code is less than half the size of the non-optimized native code. The native code of \benchname{crypt}, which heavily uses arrays, is actually made of exception checking code at 70\%.

Comparing the size of the optimized native versions with the bytecode reveals that proved native code is just slightly bigger than bytecode. The native code of \benchname{crypt} is 27\% bigger than its bytecode version. Native \benchname{scheduler} only weights 13.5\% more that its bytecode, \benchname{tcpip} 16.5\%, while \benchname{banking} is 54\% heavier. This last result is explained by the fact that, being an application and not a system componant, \benchname{banking} includes many native-to-java method invokations for calling system services. The native-to-java calling convention is costly in JITS, which artificially increases the result.

Finally, table \ref{tab:implication} details the human work required to obtain the proofs on the benchmark programs, by comparing the amount of JML code with respect to the comments-free source code of the programs. It also details how many lemmas had to be manually proved.

\begin{table}
\caption{Human work on the tested programs}
\begin{center}
  \begin{tabular}{|l|r@{\extracolsep{0.5cm}}rrr|}
    \hline
    \multirow{2}*{Program} & \multicolumn{2}{c}{Source code size (bytes)} & \multicolumn{2}{c|}{Proved lemmas}\\
    \cline{2-3} \cline{4-5} & ~~~~~~~~~Code & JML & Automatically & Manually\\
    \hline
    \benchname{crypt} & 4113 & 1882 & 227 & 77 \\
    \benchname{banking} & 11845 & 15775 & 379 & 159\\
    \benchname{scheduler} & 12539 & 3399 & 226 & 49\\
    \benchname{tcpip} & 83017 & 15379 & 2233 & 2191\\
    \hline
  \end{tabular}
\end{center}
\label{tab:implication}
\end{table}

On the three programs that are annotated for the unique purpose of our study, the JML overhead is about 30\% of the code size. The \benchname{banking} program was annotated in order to prove other properties, and because of this is made of more JML annotations than actual code. Most of the lemmas could be proved by Simplify, but a non-neglectable part needed human-assistance with Coq. The most demanding application was the TCP/IP stack. Because of its complexity, nearly half of the lemmas could not be proved automatically.

The gain in terms of memory footprint obtained using our approach is therefore real. One may also wonder whether the runtime performance of such optimized methods would be increased. We did the measurements, and only noticed a very slight, almost undetectable, improvement of the execution speed of the programs. This is explained by the fact that the exception check sites conditions are always false when evaluated, and therefore the amount of supplementary code executed is very low. The bodies of the proved runtime exception check sites are, actually, dead code that is never executed.
